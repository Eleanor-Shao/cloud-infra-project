pipeline {
    agent any
    environment {
        HADOOP_IP = "34.169.41.151"                    // 替换为你的 Hadoop 实例的外部 IP 地址
        SSH_USER = "shaoxiaowen"              // 替换为你的 SSH 用户名
        PROJECT_ID = "project-440905"                   // 替换为你的 GCP 项目 ID
        GCS_BUCKET_NAME = "hadoop-storage-bucket-project-440905" // 替换为你的 GCS 存储桶名称
        HADOOP_CLUSTER_NAME = "hadoop-cluster"          // Dataproc 集群名称
        HADOOP_REGION = "us-west1"                      // Dataproc 集群区域
    }
    stages {
        stage('SSH into Hadoop VM and Run Job') {
            steps {
                withCredentials([sshUserPrivateKey(credentialsId: 'hadoop-new-ssh-key', keyFileVariable: 'SSH_KEY')]) {
                    sh """
                        ssh -i ${SSH_KEY} -o StrictHostKeyChecking=no ${SSH_USER}@${HADOOP_IP} << EOF
                        echo "Successfully connected to Hadoop VM"
                    
                        # Authenticate and configure gcloud
                        gcloud auth activate-service-account --key-file=${GOOGLE_APPLICATION_CREDENTIALS}
                        gcloud config set project ${PROJECT_ID}
                    
                        # Submit Hadoop job to Dataproc
                        gcloud dataproc jobs submit hadoop \
                          --cluster=${HADOOP_CLUSTER_NAME} \
                          --region=${HADOOP_REGION} \
                          --jar gs://${GCS_BUCKET_NAME}/wordcount-1.0-SNAPSHOT.jar \
                          -- wordcount gs://${GCS_BUCKET_NAME}/input/input.txt \
                          gs://${GCS_BUCKET_NAME}/output/
                    
                        # Display Hadoop job results
                        gsutil cat gs://${GCS_BUCKET_NAME}/output/part-*
                        EOF
                    """

                }
            }
        }
    }
}






