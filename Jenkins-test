pipeline {
    agent any
    environment {
        HADOOP_IP = "34.169.41.151"                    // 替换为你的 Hadoop 实例的外部 IP 地址
        SSH_USER = "shaoxiaowen"                       // 替换为你的 SSH 用户名
        PROJECT_ID = "project-440905"                  // 替换为你的 GCP 项目 ID
        GCS_BUCKET_NAME = "hadoop-storage-bucket-project-440905" // 替换为你的 GCS 存储桶名称
        HADOOP_CLUSTER_NAME = "hadoop-cluster"         // Dataproc 集群名称
        HADOOP_REGION = "us-west1"                     // Dataproc 集群区域
    }
    stages {
        stage('Checkout Code') {
            steps {
                checkout([$class: 'GitSCM', branches: [[name: '*/main']], 
                          userRemoteConfigs: [[url: 'https://github.com/Eleanor-Shao/cloud-infra-project']]])
            }
        }
        stage('SSH into Hadoop VM and Upload Files') {
            steps {
                withCredentials([
                    sshUserPrivateKey(credentialsId: 'hadoop-new-ssh-key', keyFileVariable: 'SSH_KEY'),
                    file(credentialsId: 'gcp-service-account-json', variable: 'GOOGLE_APPLICATION_CREDENTIALS')
                ]) {
                    script {
                        echo "Connecting to Hadoop VM and uploading files..."

                        // Upload Google credentials to VM in /tmp directory
                        sh '''
                        scp -i ${SSH_KEY} -o StrictHostKeyChecking=no ${GOOGLE_APPLICATION_CREDENTIALS} ${SSH_USER}@${HADOOP_IP}:/tmp/gcp-service-account.json
                        '''

                        // Upload Mapper and Reducer files to /tmp directory on VM
                        sh '''
                        scp -i ${SSH_KEY} -o StrictHostKeyChecking=no mapper.py ${SSH_USER}@${HADOOP_IP}:/tmp/mapper.py
                        scp -i ${SSH_KEY} -o StrictHostKeyChecking=no reducer.py ${SSH_USER}@${HADOOP_IP}:/tmp/reducer.py
                        '''
                    }
                }
            }
        }
        stage('Submit Hadoop Job') {
            steps {
                withCredentials([sshUserPrivateKey(credentialsId: 'hadoop-new-ssh-key', keyFileVariable: 'SSH_KEY')]) {
                    // SSH into Hadoop VM and submit the Hadoop job using the files in /tmp
                    sh '''
                    ssh -i ${SSH_KEY} -o StrictHostKeyChecking=no ${SSH_USER}@${HADOOP_IP} << EOF
                    echo "Successfully connected to Hadoop VM"
                    
                    # Activate Google Cloud credentials on the VM
                    gcloud auth activate-service-account --key-file=/tmp/gcp-service-account.json
                    gcloud config set project ${PROJECT_ID}
                    
                    # Upload Mapper and Reducer to GCS bucket
                    gsutil cp /tmp/mapper.py gs://${GCS_BUCKET_NAME}/mapper.py
                    gsutil cp /tmp/reducer.py gs://${GCS_BUCKET_NAME}/reducer.py

                    # Submit the Hadoop job on Dataproc using the Mapper and Reducer scripts in GCS
                    gcloud dataproc jobs submit hadoop \
                      --cluster=${HADOOP_CLUSTER_NAME} \
                      --region=${HADOOP_REGION} \
                      --project=${PROJECT_ID} \
                      --jar=file:///usr/lib/hadoop-mapreduce/hadoop-streaming.jar \
                      -- -files=gs://${GCS_BUCKET_NAME}/mapper.py,gs://${GCS_BUCKET_NAME}/reducer.py \
                      -mapper="python3 mapper.py" \
                      -reducer="python3 reducer.py" \
                      -input=gs://${GCS_BUCKET_NAME}/input/input.txt \
                      -output=gs://${GCS_BUCKET_NAME}/output/

                    # Display the job results from GCS output folder
                    gsutil cat gs://${GCS_BUCKET_NAME}/output/part-*
                    EOF
                    '''
                }
            }
        }

    }
}

