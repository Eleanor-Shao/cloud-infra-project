pipeline {
    agent any
    environment {
        HADOOP_IP = "34.169.38.201"                    // 替换为你的 Hadoop 实例的外部 IP 地址
        SSH_USER = "root"                       // 替换为你的 SSH 用户名
        PROJECT_ID = "project-440905"                  // 替换为你的 GCP 项目 ID
        GCS_BUCKET_NAME = "dataproc-staging-us-central1-200766206890-ztjblqei" // 替换为你的 GCS 存储桶名称
        HADOOP_CLUSTER_NAME = "hadoop-cluster"         // Dataproc 集群名称
        HADOOP_REGION = "us-central1"                     // Dataproc 集群区域
    }
    stages {
        stage('Checkout Code') {
            steps {
                checkout([$class: 'GitSCM', branches: [[name: '*/main']], 
                          userRemoteConfigs: [[url: 'https://github.com/Eleanor-Shao/cloud-infra-project']]])
            }
        }
        stage('Scan') {
            steps {
                withCredentials([string(credentialsId: 'SonarQubeToken', variable: 'SONAR_TOKEN')]) {
                    withSonarQubeEnv('SonarQube') { 
                        sh '''
                        ./mvnw clean org.sonarsource.scanner.maven:sonar-maven-plugin:3.9.0.2155:sonar \
                            -Dsonar.projectKey="mapper-reducer-scan" \
                            -Dsonar.sources=mapper.py,reducer.py \
                            -Dsonar.language=py \
                            -Dsonar.token="$SONAR_TOKEN"
                        '''
                    }
                }
            }
        }
        stage('SSH into Hadoop VM and Upload Files') {
            steps {
                withCredentials([
                    sshUserPrivateKey(credentialsId: 'hadoop-new-ssh-key', keyFileVariable: 'SSH_KEY'),
                    file(credentialsId: 'gcp-service-account-json', variable: 'GOOGLE_APPLICATION_CREDENTIALS')
                ]) {
                    script {
                        echo "Connecting to Hadoop VM and uploading files..."
                        // Remove existing files before copying
                        sh """
                        ssh -i ${SSH_KEY} -o StrictHostKeyChecking=no ${SSH_USER}@${HADOOP_IP} 'rm -f /home/${SSH_USER}/gcp-service-account.json /home/${SSH_USER}/mapper.py /home/${SSH_USER}/reducer.py'
                        """

                        // Upload Google credentials to VM in /tmp directory
                        sh '''
                        scp -i ${SSH_KEY} -o StrictHostKeyChecking=no ${GOOGLE_APPLICATION_CREDENTIALS} ${SSH_USER}@${HADOOP_IP}:/home/${SSH_USER}/gcp-service-account.json
                        '''

                        // Upload Mapper and Reducer files to /tmp directory on VM
                        sh '''
                        scp -i ${SSH_KEY} -o StrictHostKeyChecking=no mapper.py ${SSH_USER}@${HADOOP_IP}:/home/${SSH_USER}/mapper.py
                        scp -i ${SSH_KEY} -o StrictHostKeyChecking=no reducer.py ${SSH_USER}@${HADOOP_IP}:/home/${SSH_USER}/reducer.py
                        '''
                    }
                }
            }
        }
        stage('Submit Hadoop Job') {
            steps {
                withCredentials([sshUserPrivateKey(credentialsId: 'hadoop-new-ssh-key', keyFileVariable: 'SSH_KEY')]) {
                    // SSH into Hadoop VM and submit the Hadoop job using the files in /tmp
                    sh """
                    ssh -i ${SSH_KEY} -o StrictHostKeyChecking=no ${SSH_USER}@${HADOOP_IP} << EOF
                    echo "Successfully connected to Hadoop VM"

                    # Activate Google Cloud credentials on the VM
                    gcloud auth activate-service-account --key-file=/home/${SSH_USER}/gcp-service-account.json
                    gcloud config set project ${PROJECT_ID}

                    # Upload Mapper and Reducer to GCS bucket
                    gsutil cp /home/${SSH_USER}/mapper.py gs://${GCS_BUCKET_NAME}/mapper.py
                    gsutil cp /home/${SSH_USER}/reducer.py gs://${GCS_BUCKET_NAME}/reducer.py

                    # Submit the Hadoop job on Dataproc using the Mapper and Reducer scripts in GCS
                    
                    gcloud dataproc jobs submit hadoop \
                    --cluster=${HADOOP_CLUSTER_NAME} \
                    --region=${HADOOP_REGION} \
                    --project=${PROJECT_ID} \
                    --jar=file:///usr/lib/hadoop/hadoop-streaming.jar \
                    -- -files=gs://${GCS_BUCKET_NAME}/mapper.py,gs://${GCS_BUCKET_NAME}/reducer.py \
                    -mapper="python3 mapper.py" \
                    -reducer="python3 reducer.py" \
                    -input=gs://${GCS_BUCKET_NAME}/input/data/* \
                    -output=gs://${GCS_BUCKET_NAME}/output/

                    # Display the job results from GCS output folder
                    gsutil cat gs://${GCS_BUCKET_NAME}/output/* > merged-output
                    cat merged-output
                    EOF
                                """
                }
            }
        }

    }
}

