pipeline {
    agent any
    environment {
        HADOOP_IP = "34.169.41.151"                    // 替换为你的 Hadoop 实例的外部 IP 地址
        SSH_USER = "shaoxiaowen"              // 替换为你的 SSH 用户名
        PROJECT_ID = "project-440905"                   // 替换为你的 GCP 项目 ID
        GCS_BUCKET_NAME = "hadoop-storage-bucket-project-440905" // 替换为你的 GCS 存储桶名称
        HADOOP_CLUSTER_NAME = "hadoop-cluster"          // Dataproc 集群名称
        HADOOP_REGION = "us-west1"                      // Dataproc 集群区域
    }
    stages {
        stage('SSH into Hadoop VM and Run Job') {
            steps {
                withCredentials([sshUserPrivateKey(credentialsId: 'hadoop-new-ssh-key', keyFileVariable: 'SSH_KEY')]) {
                    sh '''
                    ssh -i ${SSH_KEY} -o StrictHostKeyChecking=no ${SSH_USER}@${HADOOP_IP} << EOF
                    echo "Successfully connected to Hadoop VM"

                    # 执行 Hadoop 或其他任务
                    echo "GOOGLE_APPLICATION_CREDENTIALS file path: ${GOOGLE_APPLICATION_CREDENTIALS}"
                    ls -l ${GOOGLE_APPLICATION_CREDENTIALS}
                    
                    gcloud auth activate-service-account --key-file=${GOOGLE_APPLICATION_CREDENTIALS}
                    gcloud config set project ${PROJECT_ID}

                    # 提交 Hadoop 作业到 Dataproc
                    gcloud dataproc jobs submit hadoop \
                      --cluster=${HADOOP_CLUSTER_NAME} \
                      --region=${HADOOP_REGION} \
                      --jar gs://${GCS_BUCKET_NAME}/wordcount-1.0-SNAPSHOT.jar \
                      -- wordcount gs://${GCS_BUCKET_NAME}/input/input.txt \
                      gs://${GCS_BUCKET_NAME}/output/

                    # 显示 Hadoop 作业结果
                    gsutil cat gs://${GCS_BUCKET_NAME}/output/part-*
                    EOF
                    '''
                }
            }
        }
    }
}






