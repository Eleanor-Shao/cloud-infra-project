pipeline {
  agent any
  options {
    buildDiscarder(logRotator(numToKeepStr: '5'))
  }
  stages {
    stage('Scan') {
      steps {
        withCredentials([usernamePassword(credentialsId: 'SonarQubeToken', usernameVariable: 'SONAR_USER', passwordVariable: 'SONAR_PASS')]) {
          withSonarQubeEnv('SonarQube') { 
            sh './mvnw clean org.sonarsource.scanner.maven:sonar-maven-plugin:3.9.0.2155:sonar -Dsonar.projectKey="javawebapp" -Dsonar.login="$SONAR_USER" -Dsonar.password="$SONAR_PASS"'
          }
        }
      }
    }

    stage('Quality Gate') {
      steps {
        timeout(time: 1, unit: 'HOURS') {
          script {
            def qg = waitForQualityGate() // Checks SonarQube quality gate
            if (qg.status != 'OK') {
              error "Pipeline aborted due to quality gate failure: ${qg.status}"
            }
          }
        }
      }
    }
    // 格式化SonarQube结果，上传到GCS
    stage('Process SonarQube Results') {
      steps {
        script {
          // 调用SonarQube API获取分析结果
          sh 'curl -u $SONAR_USER:$SONAR_PASS "https://34.67.230.35:9000/api/issues/search?projectKeys=javawebapp" -o sonarqube_issues.json'

          // 调用Python脚本将结果格式化为MapReduce可处理的格式
          sh 'python format_sonarqube_issues.py sonarqube_issues.json sonarqube_issues.txt'

          // 上传格式化后的SonarQube结果到GCS
          sh 'gsutil cp sonarqube_issues.txt gs://hadoop-storage-bucket-final-project-437823/input-data/'
        }
      }
    }

    // 提交Hadoop MapReduce任务到Google Dataproc
    stage('Deploy to Dataproc') {
      when {
        expression {
          return currentBuild.result == null || currentBuild.result == 'SUCCESS'
        }
      }
      steps {
        withCredentials([file(credentialsId: 'gcp-service-account', variable: 'GOOGLE_APPLICATION_CREDENTIALS')]) {
          script {
            // 验证GCP并提交Hadoop MapReduce任务
            sh """
              gcloud auth activate-service-account --key-file=$GOOGLE_APPLICATION_CREDENTIALS
              gcloud dataproc jobs submit hadoop \
                --cluster=hadoop-cluster \  // 替换为你的集群名称
                --region=us-central1 \      // 替换为你的区域
                --jar=gs://hadoop-storage-bucket-final-project-437823/wordcount-1.0-SNAPSHOT.jar \  // 替换为你的JAR文件路径
                --class=WordCount \  // 替换为你的MapReduce主类的全限定名称
                -- gs://hadoop-storage-bucket-final-project-437823/input-data/ gs://hadoop-storage-bucket-final-project-437823/output-data/  // 替换为你的输入和输出路径
            """
          }
        }
      }
    }
  
    stage('Deploy to Dataproc') {
      when {
        expression {
          return currentBuild.result == null || currentBuild.result == 'SUCCESS'
        }
      }
      steps {
        withCredentials([file(credentialsId: 'gcp-service-account', variable: 'GOOGLE_APPLICATION_CREDENTIALS')]) {
          script {
            // Authenticate and submit the Hadoop job to Google Dataproc
            sh """
              gcloud auth activate-service-account --key-file=$GOOGLE_APPLICATION_CREDENTIALS
              gcloud dataproc jobs submit hadoop \
                --cluster=hadoop-cluster \  // Replace with your actual cluster name
                --region=us-central1 \      // Replace with the region from your main.tf
                --jar=gs://hadoop-storage-bucket-final-project-437823/wordcount-1.0-SNAPSHOT.jar \  // Replace with actual JAR file path
                --class=com.example.hadoop.WordCount \  // Replace with your actual MapReduce class
                -- gs://my-bucket/input-data/ gs://my-bucket/output-data/  // Replace with your input/output paths in GCS
            """
          }
        }
      }
    }
  }
}

